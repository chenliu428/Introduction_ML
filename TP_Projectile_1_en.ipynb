{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b5e8a4",
   "metadata": {},
   "source": [
    "# Project - Projectile 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48830e0",
   "metadata": {},
   "source": [
    "In this project, we aim to apply supervised machine learning techniques to a classic problem in physics: predicting the horizontal distance traveled by a projectile. The objective is to build predictive models for two related but distinct tasks. \n",
    "\n",
    "- For Task 1, the goal is to predict the horizontal distance based on the initial velocity components along the x and y axes ($v_x$ and $v_y$). \n",
    "- For Task 2, the model will predict the horizontal distance using the magnitude of the initial velocity ($v$) and the launch angle ($\\alpha$). \n",
    "\n",
    "Both tasks involve training regression models on simulated projectile data, with the ultimate aim of accurately capturing the underlying physical relationships from example data. We will see that one given system when trained for different problems shows the flexibility to adapt to different situations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d16663d",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c811b",
   "metadata": {},
   "source": [
    "Recall key ingredients in supervised machine learning:\n",
    "\n",
    "- Task (T)\n",
    "- Experience (E)\n",
    "- Performance measure (P)\n",
    "- Hypothesis Space (Machine learning model)\n",
    "- Learning Algorithm \n",
    "- Generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8488433",
   "metadata": {},
   "source": [
    "## 2. Task-1: horizontal distance from the initial velocity components ($v_x$ & $v_y$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14d18c",
   "metadata": {},
   "source": [
    "### 2.1 Define the task for Machine Learning via \"_target function_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb07686",
   "metadata": {},
   "source": [
    "In supervised machine learning, the goal is always about to infer from data (\"experience\") the relation between two sets of variables called \"**features**\" and \"**labels**\" (also called \"**targets**\") of some subject. Both **feature** and **label** can be composed by multiple quantities or variables, where each variable represents some property of the subject. \n",
    "\n",
    "> In the current project, \n",
    "> \n",
    "> - the \"subject\" under investigation is the projectile launched under the effect of gravity, \n",
    "> - the \"features\" is the pair of launching velocity components $(v_x,\\; v_y)$, and \n",
    "> - the \"label\" is the horizontal distance (i.e. along $x$) of the projectile landing position from the launching point, denoted $d$.\n",
    "\n",
    "The task meant for a supervised learning system is to return as accurately as possible the **label** when a **feature** compressing a set of pre-conventioned variables is provided. Thus from the machine's perspective, the **features** are alternatively called \"**input**\" and the **label** is alternatively called \"**output**\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fdb8ec",
   "metadata": {},
   "source": [
    "Mapping from the **feature** to the **label** for a subject in the real world is the **target function**. It is the _true association of a label to some features_, i.e. the **target function**, that is meant to be learned by a machine. \n",
    "\n",
    "The **target function**, denoted $f_T(\\cdot)$, is specified by \n",
    "\n",
    "- the form of the \"feature\" (or \"input\") -- the _domain of definition_ and the meaning for each of its variables.\n",
    "- the form of the \"label\" (or \"output\") -- the _domain of definition_ and the meaning for each of its variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158dabb",
   "metadata": {},
   "source": [
    "> In the current project, the target function $f_T(\\cdot)$ is specified by \n",
    "> - the feature domain $X\\hat{=}\\{ (v_x, v_y) | v_x \\in \\mathbb{R}^+, \\; v_y \\in \\mathbb{R}^+ \\}$ where $v_x$ and $v_y$ are respectively the horizonal and vertical components of the launching velocity, and \n",
    "> - the label domain $Y\\hat{=}\\{d|d\\in \\mathbb{R}^+\\}$ where $d$ represents the landing distance of the projectile.\n",
    "> \n",
    "> The target function is formally  $$ f_T:X\\rightarrow Y \\quad \\text{or} \\quad f_T(v_x, v_y) = d$$\n",
    ">\n",
    "> In the majority of situations, unlike the current projectile problem where the target function can be resolved (using physics), the target function is too complex to be resolved, and the task of supervised machine learning is to infer that unknown **target function** using certain techniques with available data.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcfd86",
   "metadata": {},
   "source": [
    "**_Specifying the target function defines the task intended by a machine learning system_**. It leads to crucial indications for \n",
    "\n",
    "1. The data pipline : the entire process from raw data collection to the formation of training and testing datasets ready for training and testing machine learning models. \n",
    "2. The hypothesis space: the scope of the candidating machine learning models to be used, that is models that can map from the feature domain $X$ to the label domain $Y$.\n",
    "3. The performance measure definition: when a target $\\hat t$ output by a machine learning system mismatches the true target $t$, one needs to specify the so called **loss function**, denoted $L(\\cdot, \\cdot)$ mapping $(t,\\hat t)\\in Y^2$ to some domain of scalar usually $\\mathbb{R}^+$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246e610",
   "metadata": {},
   "source": [
    "### 2.2 Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9e8a4",
   "metadata": {},
   "source": [
    "Once the target function is well specified, also clarified is the final product of the data pipline, i.e. an ensemble of observed \"feature-target\" pairs. In practice, if no raw data is provided, one needs to designe the data collection and cleaning up process in order to produce the ready-to-use \"feature-target\" pairs, or else one shall transform the raw data into the form of \"feature-target\" pairs required by the target function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933d508",
   "metadata": {},
   "source": [
    "In this current project, the final product of data pipline, i.e. feature-target pairs, is prepared ready in `/training_set_1.dat` for you to proceed further machine learning steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc450814",
   "metadata": {},
   "source": [
    "Once the training dataset of feature-target pairs is ready, it is helful to perform the so-called \"**data exploration**\" to gain insights of the connections among all variables (in both feature and target) for a wise direction in picking up machine learning models. \n",
    "\n",
    "**Data exploration**, in principle, should be directed by generic questions about the internal mechanism underlying the subject, which varies with case and approache. Here, we will go over some common procedures for data exploration through a series of exercices and derive some insights for picking up machine learning models for the current project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce6a3a",
   "metadata": {},
   "source": [
    "#### **Exercice 2.2.1**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef3bc3b",
   "metadata": {},
   "source": [
    "Load the followin data files with `numpy.loadtxt` function:\n",
    "  - \"training_set_1.dat\"\n",
    "  - \"test_set_1.dat\" \n",
    "\n",
    "1. Explore the loaded data structure. How many entries \"feature-target\" pairs in each dataset?\n",
    "2. Explore the header of the data files, and determine which columns are the inputs (features) and which columns are the output (targets)?\n",
    "\n",
    "Using the code cell below. Reminder: you can use `help()`, `dir()` and `type()` for the manual of new objects in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load data files\n",
    "# train1 = np.loadtxt(..)\n",
    "\n",
    "# data structure, how many columns and rows?\n",
    "\n",
    "# header of the data files, which columns are the inputs and which are the output?\n",
    "\n",
    "# print the first 5 entries of the training set and the test set    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a951699",
   "metadata": {},
   "source": [
    "#### **Exercice 2.2.2** Distribution of the input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb929e4",
   "metadata": {},
   "source": [
    "With the help of `matplotlib`, exploring the follwoing aspect of the input variables (feature variables) in `training_set_1.dat`:\n",
    "\n",
    "1. For each variable in the feature, what is its empirical distribution? Hint: one can plot the histogram using `matplotlib.pyplot.hist`.\n",
    "2. Is there a most probable value each input variable may take?\n",
    "3. Estimate the expectation of each input variable.\n",
    "4. Estimate the fluctuation of each input variable around its expectation.\n",
    "5. How to transform an input variable such that it has zero expectation and unity standard deviation? Such a transformation is called \"normalisation\".\n",
    "\n",
    "Use the following code cell for this exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib widget  # for interactive plotting\n",
    "\n",
    "# load the training set and declare input variables\n",
    "# train1 = np.loadtxt(..)\n",
    "# vx = \n",
    "# vy = \n",
    "\n",
    "# Distribution of the input variables by ploting the histogram of vx and vy\n",
    "# plt.hist(..)\n",
    "\n",
    "# Estimate the expectation\n",
    "\n",
    "# Estimate the fluctuation\n",
    "\n",
    "# Transform the input variable such that it has zero expectation and unity standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38373076",
   "metadata": {},
   "source": [
    "#### **Exercice 2.2.3** Correlation among the input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ef2a0",
   "metadata": {},
   "source": [
    "For the same input variables studied in the previous exercise, are these input variables correlated? Is the value of one input variable informative for the value of other input variables? Hint: one may plot one variables against another to reveal sign of mutual dependence.\n",
    "\n",
    "Use the following code cell for the investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61846063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib widget  # for interactive plotting\n",
    "\n",
    "# Correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6d8e7",
   "metadata": {},
   "source": [
    "#### **Exercice 2.2.4** Target value distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b083fa73",
   "metadata": {},
   "source": [
    "Always with the data in `training_set_1.dat`, now we turn to investigate statistical properties of the target variables. Using the same technique, explore the following aspect of the target variables\n",
    "\n",
    "1. The empirical distribution of the target variable.\n",
    "2. Is there a most probable value for the target variable?\n",
    "3. Estimate the expectation.\n",
    "4. Estimate the fluctuation.\n",
    "5. Normalise the target variables.\n",
    "\n",
    "Use the following code cell for this exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib widget  # for interactive plotting\n",
    "\n",
    "# Target value distribution\n",
    "\n",
    "# The most probable value for the target variable\n",
    "\n",
    "# Estimate the expectation\n",
    "\n",
    "# Estimate the fluctuation\n",
    "\n",
    "# Normalise the target variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce743c",
   "metadata": {},
   "source": [
    "#### **Exercice 2.2.5** How does the target depend on the input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7af606",
   "metadata": {},
   "source": [
    "Now we turn to investigate how does the target depend on the input variables in `training_set_1.dat`.\n",
    "\n",
    "1. For each input variables, explore how does the target variable depend on the input variable using graphics. \n",
    "2. Compute the correlation coefficient between the target variable and each of the input variables.\n",
    "3. Summarize your results for Q1 and Q2.\n",
    "4. How to reveal the dependence of the target on both of the input variables? Hint: make a scattering plot where each point position represents the inputs and use its color for the target.\n",
    "5. What is your insights from Q4? What kind of form you may guess for the target function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib widget  # for interactive plotting\n",
    "\n",
    "# Scattering plot of target vs each of the input variables\n",
    "\n",
    "# Scattering plot of target vs both of the input variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac42c1",
   "metadata": {},
   "source": [
    "### 2.3 Hypothesis Space, Performance Metric, and Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bb853",
   "metadata": {},
   "source": [
    "In one phrase, **Learning Algorithm** searches the function (also called \"hypothesis\" or \"model\") within a domain defined by the **Hypothesis Space**, that is with the optimal **performance metric** to approximates the target function. \n",
    "\n",
    "We shall go through the key concepts one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb7121",
   "metadata": {},
   "source": [
    "- A **hypothesis space**, denoted $\\mathcal{H}$, defines a set of possible functions (or models) $h(\\cdot)$ from which an \"optimal\" one can be chosen to perform the task of the target function $f_T$, i.e. mapping every feature in $X$ to a target in $Y$. For example, for some target function $f_T:\\mathbb{R}\\rightarrow \\mathbb{R}$, one may propose an hypothesis space $$\\mathcal{H}=\\{h(x)=ax^{p}| a\\in \\mathbb{R}, p\\in\\mathbb{Z}\\}\\quad .$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f283e",
   "metadata": {},
   "source": [
    "- The \"optimal\" function $h^*(\\cdot)$ (within in the scope of $\\mathcal{H}$) is chosen against a customery **performance metric** that quantifies how well a function $h(\\cdot)$ approximate the target function $f_T(\\cdot)$. This concept compresses two ingredients:\n",
    "  - **loss function**\n",
    "  - Loss over the population -- **Expected loss**.\n",
    "\n",
    "- **loss function**, denoted $L$, associates a degree of \"loss\", i.e. a scalar, to a pair of the true target $y = f_T(x)$ and the predicted target $\\hat{y} = h(x)$. Formally $L(\\hat{y}, y):Y^2\\rightarrow \\mathbb{R}$. The loss function basically tells how bad it is if a predicted target is $\\hat y$ while the true target is $y$. Generally speaking, you want a hypothesis resulting in a small value of the loss function. Here are two common examples of loss functions $L(\\hat y, y)=|\\hat y - y|$ and $L(\\hat y, y)=(\\hat y - y)^2$ (for the target domain $Y=\\mathbb{R}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5207b",
   "metadata": {},
   "source": [
    "#### **Exercice 2.3.1: Comparing two functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc6740",
   "metadata": {},
   "source": [
    "Assuming the target function $f_T:\\mathbb{R}\\rightarrow \\mathbb{R}$, how to compare the following two functions $h_1(x)=2x$ and $h_2(x)=2x^2$ using the same performance metric $L(\\hat y, y)=(\\hat y -y)^2$? \n",
    "\n",
    "A sample of feature-target pairs are collected from certain population-1 and stored in the file `population_1.dat`. \n",
    "\n",
    "1. How many feature-target pairs are there in this sample?\n",
    "2. Construct a numpy array of predicted targets by $h_1$ and a numpy array of predicted targets by $h_2$. Name these two arrays `y1` and `y2` respectively.\n",
    "3. Construct a scattering plot of the loss of $h_1$ as a function of the collected features. Do the same for $h_2$ on the same figure.\n",
    "4. According to the scattering plot in Q3, does $h_1$ always outperform or underperform $h_2$ ?\n",
    "5. What is the empirical distribution of the feature in this population?\n",
    "6. Take into account of the feature distribution, can you gues which function, $h_1$ or $h_2$, performs better over the entire population?\n",
    "7. Come up with a measure that quantifies the performance of a function $h$ over the entire population with respect to a loss function. Apply this measure to $h_1$ and $h_2$ with the loss $L(\\hat y, y)=(\\hat y - y)^2$. Print the result, which one is better?\n",
    "\n",
    "Use the following code cell for this exercice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077fa2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.clf()\n",
    "%matplotlib widget\n",
    "\n",
    "xs = np.linspace(0, 10, 101)\n",
    "\n",
    "h1 = lambda x: 2*x\n",
    "h2 = lambda x: 2*x**2\n",
    "\n",
    "loss_func = lambda y_true, y_pred: (y_true - y_pred)**2\n",
    "\n",
    "d1 = np.loadtxt('./data/projectile_1/population_1.dat')\n",
    "\n",
    "# how many feature-target pairs are there in this sample?\n",
    "\n",
    "# construct numpy arrays of predicted targets by h1 and h2\n",
    "# y1 = \n",
    "# y2 = \n",
    "\n",
    "# scattering plot of the loss of h1 as a function of the collected features in population 1\n",
    "\n",
    "# scattering plot of the loss of h2 as a function of the collected features in population 1\n",
    "\n",
    "# histogram of the feature in this population\n",
    "\n",
    "# print the result of the performance measure for h1 and h2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a487e1",
   "metadata": {},
   "source": [
    "#### **Exercice 2.3.2: Comparing again for different population**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a793ec78",
   "metadata": {},
   "source": [
    "Now we investigate a different population `population_2.dat`. It is given that both pupolation 1 and population 2 admit the same target function. Compare again the performance of $h_1$ and $h_2$ but this time over population 2. \n",
    "\n",
    "1. What is distribution of feature in population 2?\n",
    "2. Compare the two populations in terms of the feature distribution.\n",
    "3. Guess over the entire population 2, which one, $h_1$ or $h_2$, will perform better?\n",
    "4. Verify your guess using the measure defined in the previous exercice Q7.\n",
    "5. What can you draw as conclusion on how to properly comparing the performance different functions? \n",
    "\n",
    "Use the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02baf333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.clf()\n",
    "%matplotlib widget\n",
    "\n",
    "xs = np.linspace(0, 10, 101)\n",
    "\n",
    "h1 = lambda x: 2*x\n",
    "h2 = lambda x: 2*x**2\n",
    "\n",
    "loss_func = lambda y_true, y_pred: (y_true - y_pred)**2\n",
    "\n",
    "d2 = np.loadtxt('./data/projectile_1/population_2.dat')\n",
    "\n",
    "# what is distribution of feature in population 2?\n",
    "\n",
    "# compare the two populations in terms of the feature distribution, mean, standard deviation, etc.\n",
    "\n",
    "# verify your guess using the measure defined in the previous exercice Q7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a5389",
   "metadata": {},
   "source": [
    "- The loss function only assign a degree of badness to an instance of feature-target pair when a hypothesis (function) $h$ is applied. However, we want to optimise our function (within the hypothesis space) such that, it performs well over all possibly encountered features. This is why we introduce the **Expected loss** to evaluate the \"badness\" of a funtion $h$ over the entire population of features. Since one disposes only the observed data (i.e. training data) to gain some knowledge about the population, one has to use the **empirical loss** defined as $$ \\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^n L(h(x_i), y_i)$$ where $(x_1,y_1),(x_2, y_2),\\ldots,(x_n,y_n)$ are observed feature-target pairs, to estimate the **expected loss**. \n",
    "\n",
    "  **Remark**\n",
    "   - It is however important to be clear that **empiracal loss**, which estimate the performace in sample, is not **expected loss**, which measures the performance over the entire population. The only way make these two quantitiy equal, is to make $n$ goes to infinity (given that feature-target pairs are independently generated). \n",
    "   - This remark has important implications in how well a function $h$ optimised over the training samples can perform out of the sample, i.e. the expected loss. When optimisation is overly done to optimise the **empirical loss**, it can go against generalisation such that the **expected loss** is not optimal. A technique called **regularisation** is introduced for this issue, which will be discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b6680",
   "metadata": {},
   "source": [
    "- **Learning algorithm** is the specific computational scheme to search for the optimal _parameters_ from the chosen hypothesis space. For functions in the previous example of hypothesis space having form $h(x)=ax^p$, the parameters are $a$ and $p$. A learning alogrithm in this case is series of concrete computational operations that, when it finishes, returns the $a$ and $p$ for optimal **empirical loss** (or ideally **expected loss**). \n",
    "\n",
    "  When an analytical solution is not possible, an iterative loop to approach the optimal parameters must be invoked in **Learning algorithm**. In this case, a learning algoritm can viewed as a discrete dynamical system (because computation is discrete) in the parameter space. \n",
    "\n",
    "  - A dynamical system, is a set of rules to update its configuration based only on its configuration. Take $h(x)=ax^p$ as an example, a hypothesis (function) is completely determined by the pair $(a, p)$, which is called configuration. The variation of $a$ and $p$ is solely determined by  $a$ and $p$, for example $\\Delta a = (a^2 + 3p)\\times \\ell$ and $\\Delta p = (-a+p)\\times \\ell$, where $\\ell$ sets the magnitude of each update.  The update scheme simply reads $a\\rightarrow a+ \\Delta a,\\; p\\rightarrow p + \\Delta p$. As such, some initial position $(a,p)$ draws a trajectory after multiple steps of update. In particular, when $\\ell\\rightarrow 0$, one end up with a system of differential equations.\n",
    "\n",
    "  - A learning algorithm is a such a dynamical system with a set of update rules that moves the configuration such that the empirical loss is reduced. This is the essential idea of **gradient descent** -- the most common idea for learning algorithms dealing with **emprical loss** in supervised machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e412a",
   "metadata": {},
   "source": [
    "#### **Exercice 2.3.3 Gradient Descent in 1 dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bff1d1",
   "metadata": {},
   "source": [
    "Assuming that we search the optimal function from the hypothesis space $\\mathcal{H} = \\{h(x)=kx|k\\in\\mathbb{R}\\}$. That is the optimal slope $k$ for minimizing some empirical loss. Assume also the emprical loss is given by $L(k) = k^2-5k+6$. We search for the optimal model identified by $k$. \n",
    "\n",
    "1. How to find analytically the optimal $k^*$ for this empirical loss? What is the result?\n",
    "2. What is the derivative of $L$ with respect to $k$?\n",
    "3. What is the sign of the derivative when $k$ is smaller than the optimal $k^*$? and when $k>k^*$?\n",
    "4. How does the magnitude of the derivative vary when $k$ approaches $k^*$ from the left? and from the right?\n",
    "5. Set up a rule for updating $k$ with a small magnitude of increment $\\ell$, such that where ever is $k$, the increment will be in the direction to approach $k^*$ from the current $k$.\n",
    "6. How to make the increment rule adaptive such that, the increment will \"slow down\" in each one move when $k$ is getting closer to $k^*$? Hint: derivative magnitude.\n",
    "7. Implement this learning algorithm with Python. \n",
    "8. Plot the $k-k^*$ as a function of the iteration steps until $k$ becomes more or less stable with different values of $\\ell$. What is the effect of $\\ell$? Hint: in terms of steps to converge, and in terms of precision to $k^*$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "004d0ccd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "352b56ea",
   "metadata": {},
   "source": [
    "    - dd "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a7f6cc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e0243e17",
   "metadata": {},
   "source": [
    "### 2.3 Mini linear regression with \"scikit-learn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea6906",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c388304f",
   "metadata": {},
   "source": [
    "#### Ordinary Least Square Regression (without feature engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Use training data (vx, vy) to predict dx\n",
    "X_train = train1[:, :2]\n",
    "y_train = train1[:, 2]\n",
    "X_test = test1[:, :2]\n",
    "y_test = test1[:, 2]\n",
    "\n",
    "# Initialize OLS regression model\n",
    "ols = LinearRegression()\n",
    "\n",
    "# Train OLS regression model\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = ols.predict(X_test)\n",
    "\n",
    "# Compute performance measure (mean squared error)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(\"Mean Squared Error (on test set):\", mse)\n",
    "\n",
    "# Optionally, show model coefficients\n",
    "print(\"Learned coefficients:\", ols.coef_)\n",
    "print(\"Learned intercept:\", ols.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c7dc6f",
   "metadata": {},
   "source": [
    "#### Ordinary Least Square Regression with feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9265744",
   "metadata": {},
   "source": [
    "For a more complex model, one needs to construct more tailored features from the raw input features — a process known as _feature engineering_.\n",
    "\n",
    "We will explore a specific type of feature engineering -- polynomial feature expansion by _taking powers and cross-products of the original features, allowing models to capture nonlinear relationships within the data_.\n",
    "\n",
    "We will perform the polynomial expansion up to order 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675af983",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a feature maxtrix in analogy with X_train before, but this time with polynomial features up to order 3 constructed from the raw input feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692e1764",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Repeat the traininig and testing process as before, and print the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7766cfb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Use training data (vx, vy) to predict dx, same as above\n",
    "X_train = train1[:, :2]\n",
    "y_train = train1[:, 2]\n",
    "X_test = test1[:, :2]\n",
    "y_test = test1[:, 2]\n",
    "\n",
    "# Initialize Ridge regression model\n",
    "ridge = Ridge(alpha=1.0)\n",
    "\n",
    "# Train Ridge regression model with penalty coefficient alpha=1.0\n",
    "ridge.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "# Compute performance measure (mean squared error)\n",
    "mse_ridge = mean_squared_error(y_test, y_pred_ridge)\n",
    "print(\"Ridge Regression Mean Squared Error (on test set):\", mse_ridge)\n",
    "\n",
    "# Optionally, show model coefficients\n",
    "print(\"Ridge Regression learned coefficients:\", ridge.coef_)\n",
    "print(\"Ridge Regression learned intercept:\", ridge.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a306d8ea",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code for generating training and test sets ./data/projectile_1\n",
    "g = 9.81\n",
    "\n",
    "def distance_vx_vy(vx, vy):\n",
    "    return vx*(vy/g)*2\n",
    "    \n",
    "def distance_v_alpha(v, alpha):\n",
    "    return v*np.sin(2*alpha)*(v/g)\n",
    "\n",
    "n_train = 10000\n",
    "n_test = 2000\n",
    "\n",
    "vx = np.random.normal(80, 15, n_train + n_test)    \n",
    "vy = np.random.normal(80, 15, n_train + n_test)\n",
    "\n",
    "v = np.sqrt(vx**2 + vy**2)\n",
    "alpha = np.arctan2(vy, vx)\n",
    "\n",
    "dx_std = 10\n",
    "dx = vx*(vy/g)*2 + np.random.normal(0, dx_std, n_train + n_test)\n",
    "\n",
    "\n",
    "train1 = np.zeros((n_train, 3))\n",
    "train1[:,0] = vx[:n_train]\n",
    "train1[:,1] = vy[:n_train]\n",
    "train1[:,2] = dx[:n_train]\n",
    "\n",
    "test1 = np.zeros((n_test, 3))\n",
    "test1[:,0] = vx[n_train:n_train+n_test]\n",
    "test1[:,1] = vy[n_train:n_train+n_test]\n",
    "test1[:,2] = dx[n_train:n_train+n_test]\n",
    "\n",
    "train2 = np.zeros((n_train, 3))\n",
    "train2[:,0] = v[:n_train]\n",
    "train2[:,1] = alpha[:n_train]\n",
    "train2[:,2] = dx[:n_train]\n",
    "\n",
    "test2 = np.zeros((n_test, 3))\n",
    "test2[:,0] = v[n_train:n_train+n_test]\n",
    "test2[:,1] = alpha[n_train:n_train+n_test]\n",
    "test2[:,2] = dx[n_train:n_train+n_test]\n",
    "\n",
    "\n",
    "# np.savetxt('./data/projectile_1/training_set_1.dat', train1, header='vx vy dx')\n",
    "# np.savetxt('./data/projectile_1/training_set_2.dat', train2, header='v alpha dx')\n",
    "# np.savetxt('./data/projectile_1/test_set_1.dat', test1, header='vx vy dx')\n",
    "# np.savetxt('./data/projectile_1/test_set_2.dat', test2, header='v alpha dx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df71a504",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ca053fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sc = plt.scatter(vx[:1000], vy[:1000], c=dx[:1000], marker='o')\n",
    "plt.colorbar(sc, label='dx')\n",
    "plt.xlabel('vx')\n",
    "plt.ylabel('vy')\n",
    "plt.title('vy vs vx colored by dx (first 1000 points)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b39c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vx, dx, 'o', alpha=0.2, mec='none' )\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('vx')\n",
    "plt.ylabel('dx')\n",
    "plt.title('dx vs vx')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80305028",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 101)\n",
    "y1 = 2*x\n",
    "y2 = 2*x**2\n",
    "z = 2*x**1.5\n",
    "\n",
    "plt.plot(x, (y1-z)**2, label='y1')\n",
    "plt.plot(x, (y2-z)**2, label='y2')\n",
    "plt.legend()\n",
    "\n",
    "s1 = np.random.exponential(0.1,1000)\n",
    "t1 = 2*s1**1.5 \n",
    "s2 = np.random.exponential(1.0,1000)\n",
    "t2 = 2*s2**1.5\n",
    "\n",
    "plt.plot(s1, t1, 'o', alpha=0.2, mec='none')\n",
    "plt.plot(s2, t2, 'o', alpha=0.2, mec='none')\n",
    "plt.show()\n",
    "\n",
    "s = s1\n",
    "t = t1\n",
    "h1 = 2*s\n",
    "h2 = 2*s**2\n",
    "plt.figure()\n",
    "plt.plot(t, h1, 'o', alpha=0.2, mec='none')\n",
    "plt.plot(t, h2, 'o', alpha=0.2, mec='none')\n",
    "plt.show()\n",
    "\n",
    "print('h1', np.mean(np.abs(h1-t)))\n",
    "print('h2', np.mean(np.abs(h2-t)))\n",
    "\n",
    "d1 = np.zeros((len(s1), 2))\n",
    "d2 = np.zeros((len(s2), 2))\n",
    "d1[:,0] = s1\n",
    "d1[:,1] = t1\n",
    "d2[:,0] = s2\n",
    "d2[:,1] = t2\n",
    "np.savetxt('./data/projectile_1/population_1.dat', d1, header='x y')\n",
    "np.savetxt('./data/projectile_1/population_2.dat', d2, header='x y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d201a2d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

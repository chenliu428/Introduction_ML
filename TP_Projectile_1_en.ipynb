{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89b5e8a4",
   "metadata": {},
   "source": [
    "# Project - Projectile 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb472001",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d48830e0",
   "metadata": {},
   "source": [
    "In this project, we aim to apply supervised machine learning techniques to a classic problem in physics: predicting the horizontal distance traveled by a projectile. The objective is to build predictive models for two related but distinct tasks. \n",
    "\n",
    "- For Project 1, the goal is to predict the horizontal distance based on the initial velocity components along the x and y axes ($v_x$ and $v_y$). \n",
    "- For Project 2, the model will predict the horizontal distance using the magnitude of the initial velocity ($v$) and the launch angle ($\\theta$). \n",
    "\n",
    "Both tasks involve training regression models on simulated projectile data, with the ultimate aim of accurately capturing the underlying physical relationships from example data. We will see that one given system when trained for different problems shows the flexibility to adapt to different situations.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d16663d",
   "metadata": {},
   "source": [
    "## 1. Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e6c811b",
   "metadata": {},
   "source": [
    "Recall key ingredients in supervised machine learning:\n",
    "\n",
    "- Task (T)\n",
    "- Experience (E)\n",
    "- Performance measure (P)\n",
    "- Hypothesis Space (Machine learning model)\n",
    "- Learning Algorithm \n",
    "- Generalisation\n",
    "\n",
    "We will go through all these elements in detail with Project 1, and then apply those concepts and techniques to Project 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8488433",
   "metadata": {},
   "source": [
    "## 2. Project-1: horizontal distance from the initial velocity components ($v_x$ & $v_y$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f14d18c",
   "metadata": {},
   "source": [
    "### 2.1 Define the task for Machine Learning via \"_target function_\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdb07686",
   "metadata": {},
   "source": [
    "In supervised machine learning, the goal is always to infer from data (\"experience\") the relation between two sets of variables called \"**features**\" and \"**labels**\" (also called \"**targets**\") of some subject. Both **feature** and **label** can be composed by multiple quantities or variables, where each variable represents some property of the subject. \n",
    "\n",
    "> In the current project, \n",
    "> \n",
    "> - the \"subject\" under investigation is the projectile launched under the effect of gravity, \n",
    "> - the \"features\" is the pair of launching velocity components $(v_x,\\; v_y)$, and \n",
    "> - the \"label\" is the horizontal distance (i.e. along $x$) of the projectile landing position from the launching point, denoted $d$.\n",
    "\n",
    "The task meant for a supervised learning system is to return as accurately as possible the **label** when a **feature** compressing a set of pre-conventioned variables is provided. Thus from the machine's perspective, the **features** are alternatively called \"**input**\" and the **label** is alternatively called \"**output**\". "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fdb8ec",
   "metadata": {},
   "source": [
    "Mapping from the **feature** to the **label** for a subject in the real world is the **target function**. It is the _true association of a label to some features_, i.e. the **target function**, that is meant to be learned by a machine. \n",
    "\n",
    "The **target function**, denoted $f_T(\\cdot)$, is specified by \n",
    "\n",
    "- the form of the \"feature\" (or \"input\") -- the _domain of definition_ and the meaning for each of its variables.\n",
    "- the form of the \"label\" (or \"output\") -- the _domain of definition_ and the meaning for each of its variables. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1158dabb",
   "metadata": {},
   "source": [
    "> In the current project, the target function $f_T(\\cdot)$ is specified by \n",
    "> - the feature domain $X\\hat{=}\\{ (v_x, v_y) | v_x \\in \\mathbb{R}^+, \\; v_y \\in \\mathbb{R}^+ \\}$ where $v_x$ and $v_y$ are respectively the horizonal and vertical components of the launching velocity, and \n",
    "> - the label domain $Y\\hat{=}\\{d|d\\in \\mathbb{R}^+\\}$ where $d$ represents the landing distance of the projectile.\n",
    "> \n",
    "> The target function is formally  $$ f_T:X\\rightarrow Y \\quad \\text{or} \\quad f_T(v_x, v_y) = d$$\n",
    ">\n",
    "> In the majority of situations, unlike the current projectile problem where the target function can be resolved (using physics), the target function is too complex to be resolved, and the task of supervised machine learning is to infer that unknown **target function** using certain techniques with available data.   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26fcfd86",
   "metadata": {},
   "source": [
    "**_Specifying the target function defines the task intended for a machine learning system_**. It leads to crucial indications for \n",
    "\n",
    "1. The data pipeline : the entire process from raw data collection to the formation of training and testing datasets ready for training and testing machine learning models. \n",
    "2. The hypothesis space: the scope of the candidating machine learning models to be used, that is models that can map from the feature domain $X$ to the label domain $Y$.\n",
    "3. The performance measure definition: when a target $\\hat t$ output by a machine learning system mismatches the true target $t$, one needs to specify the so called **loss function**, denoted $L(\\cdot, \\cdot)$ mapping $(t,\\hat t)\\in Y^2$ to some domain of scalar usually $\\mathbb{R}^+$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a246e610",
   "metadata": {},
   "source": [
    "### 2.2 Data exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7a9e8a4",
   "metadata": {},
   "source": [
    "Once the target function is well specified, also clarified is the final product of the data pipeline, i.e. an ensemble of observed \"feature-target\" pairs. In practice, if no raw data is provided, one needs to design the data collection and cleaning up process in order to produce the ready-to-use \"feature-target\" pairs, or else one shall transform the raw data into the form of \"feature-target\" pairs required by the target function. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7933d508",
   "metadata": {},
   "source": [
    "In this current project, the final product of data pipeline, i.e. feature-target pairs, is prepared ready in `/training_set_1.dat` for you to proceed further machine learning steps."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc450814",
   "metadata": {},
   "source": [
    "Once the training dataset of feature-target pairs is ready, it is helpful to perform the so-called \"**data exploration**\" to gain insights of the connections among all variables (in both feature and target) for a wise direction in picking up machine learning models. \n",
    "\n",
    "**Data exploration**, in principle, should be directed by generic questions about the internal mechanism underlying the subject, which varies with case and approach. Here, we will go over some common procedures for data exploration through a series of exercises and derive some insights for picking up machine learning models for the current project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acce6a3a",
   "metadata": {},
   "source": [
    "#### **Exercise 2.2.1** Load data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef3bc3b",
   "metadata": {},
   "source": [
    "Load the following data files with `numpy.loadtxt` function:\n",
    "  - \"training_set_1.dat\"\n",
    "  - \"test_set_1.dat\" \n",
    "\n",
    "1. Explore the loaded data structure. How many entries \"feature-target\" pairs in each dataset?\n",
    "2. Explore the header of the data files, and determine which columns are the inputs (features) and which columns are the output (targets)?\n",
    "\n",
    "Using the code cell below. Reminder: you can use `help()`, `dir()` and `type()` for the manual of new objects in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9579563c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Load data files\n",
    "# train1 = np.loadtxt(..)\n",
    "\n",
    "# data structure, how many columns and rows?\n",
    "\n",
    "# header of the data files, which columns are the inputs and which are the output?\n",
    "\n",
    "# print the first 5 entries of the training set and the test set    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a951699",
   "metadata": {},
   "source": [
    "#### **Exercise 2.2.2** Distribution of the input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fb929e4",
   "metadata": {},
   "source": [
    "With the help of `matplotlib`, exploring the following aspect of the input variables (feature variables) in `training_set_1.dat`:\n",
    "\n",
    "1. For each variable in the feature, what is its empirical distribution? Hint: one can plot the histogram using `matplotlib.pyplot.hist`.\n",
    "2. Is there a most probable value each input variable may take?\n",
    "3. Estimate the expectation of each input variable.\n",
    "4. Estimate the fluctuation of each input variable around its expectation.\n",
    "5. How to transform an input variable such that it has zero expectation and unity standard deviation? Such a transformation is called \"normalisation\".\n",
    "\n",
    "Use the following code cell for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff2b567c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib widget  # for interactive plotting\n",
    "\n",
    "# load the training set and declare input variables\n",
    "# train1 = np.loadtxt(..)\n",
    "# vx = \n",
    "# vy = \n",
    "\n",
    "# Distribution of the input variables by ploting the histogram of vx and vy\n",
    "# plt.hist(..)\n",
    "\n",
    "# Estimate the expectation\n",
    "\n",
    "# Estimate the fluctuation\n",
    "\n",
    "# Transform the input variable such that it has zero expectation and unity standard deviation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38373076",
   "metadata": {},
   "source": [
    "#### **Exercise 2.2.3** Correlation among the input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "828ef2a0",
   "metadata": {},
   "source": [
    "For the same input variables studied in the previous exercise, are these input variables correlated? Is the value of one input variable informative for the value of other input variables? Hint: one may plot one variables against another to reveal sign of mutual dependence.\n",
    "\n",
    "Use the following code cell for the investigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61846063",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib widget  # for interactive plotting\n",
    "\n",
    "# Correlation analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc6d8e7",
   "metadata": {},
   "source": [
    "#### **Exercise 2.2.4** Target value distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b083fa73",
   "metadata": {},
   "source": [
    "Always with the data in `training_set_1.dat`, now we turn to investigate statistical properties of the target variables. Using the same technique, explore the following aspect of the target variables\n",
    "\n",
    "1. The empirical distribution of the target variable.\n",
    "2. Is there a most probable value for the target variable?\n",
    "3. Estimate the expectation.\n",
    "4. Estimate the fluctuation.\n",
    "5. Normalise the target variables.\n",
    "\n",
    "Use the following code cell for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3fbf86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib widget  # for interactive plotting\n",
    "\n",
    "# Target value distribution\n",
    "\n",
    "# The most probable value for the target variable\n",
    "\n",
    "# Estimate the expectation\n",
    "\n",
    "# Estimate the fluctuation\n",
    "\n",
    "# Normalise the target variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ce743c",
   "metadata": {},
   "source": [
    "#### **Exercise 2.2.5** How does the target depend on the input variables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d7af606",
   "metadata": {},
   "source": [
    "Now we turn to investigate how does the target depend on the input variables in `training_set_1.dat`.\n",
    "\n",
    "1. For each input variables, explore how does the target variable depend on the input variable using graphics. \n",
    "2. Compute the correlation coefficient between the target variable and each of the input variables.\n",
    "3. Summarize your results for Q1 and Q2.\n",
    "4. How to reveal the dependence of the target on both of the input variables? Hint: make a scattering plot where each point position represents the inputs and use its color for the target.\n",
    "5. What is your insights from Q4? What kind of form you may guess for the target function?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b203b5c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib widget  # for interactive plotting\n",
    "\n",
    "# Scattering plot of target vs each of the input variables\n",
    "\n",
    "# Scattering plot of target vs both of the input variables\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8ac42c1",
   "metadata": {},
   "source": [
    "### 2.3 Hypothesis Space, Performance Metric, and Learning Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac5bb853",
   "metadata": {},
   "source": [
    "In one phrase, **Learning Algorithm** searches the function (also called \"hypothesis\" or \"model\") within a domain defined by the **Hypothesis Space**, that is with the optimal **performance metric** to approximates the target function. \n",
    "\n",
    "We shall go through the key concepts one by one."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40cb7121",
   "metadata": {},
   "source": [
    "- A **hypothesis space**, denoted $\\mathcal{H}$, defines a set of possible functions (or models) $h(\\cdot)$ from which an \"optimal\" one can be chosen to perform the task of the target function $f_T$, i.e. mapping every feature in $X$ to a target in $Y$. For example, for some target function $f_T:\\mathbb{R}\\rightarrow \\mathbb{R}$, one may propose an hypothesis space $$\\mathcal{H}=\\{h(x)=ax^{p}| a\\in \\mathbb{R}, p\\in\\mathbb{Z}\\}\\quad .$$ \n",
    "\n",
    "  Indeed, quite often as this example, the **hypothesis space** can be viewed as a set of functions of some specific form with varying parameters, and the **hypothesis space** is equivalently represented by the space of all possible parameter settings. In the last example, $\\mathcal{H} \\leftrightarrow \\{(a,p)|(a,p)\\in \\mathbb{R}\\times\\mathbb{Z}\\} \\;(= \\mathbb{R}\\times\\mathbb{Z})$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "834f283e",
   "metadata": {},
   "source": [
    "- The \"optimal\" function $h^*(\\cdot)$ (within the scope of $\\mathcal{H}$) is chosen against a customary **performance metric** that quantifies how well a function $h(\\cdot)$ approximates the target function $f_T(\\cdot)$. This concept comprises two ingredients:\n",
    "  - **loss function**\n",
    "  - Loss over the population -- **Expected loss**.\n",
    "\n",
    "- **loss function**, denoted $L$, associates a degree of \"loss\", i.e. a scalar, to a pair of the true target $y = f_T(x)$ and the predicted target $\\hat{y} = h(x)$. Formally $L(\\hat{y}, y):Y^2\\rightarrow \\mathbb{R}$. The loss function basically tells how bad it is if a predicted target is $\\hat y$ while the true target is $y$. Generally speaking, you want a hypothesis resulting in a small value of the loss function. Here are two common examples of loss functions $L(\\hat y, y)=|\\hat y - y|$ and $L(\\hat y, y)=(\\hat y - y)^2$ (for the target domain $Y=\\mathbb{R}$)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "daf5207b",
   "metadata": {},
   "source": [
    "#### **Exercise 2.3.1: Comparing two functions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4dc6740",
   "metadata": {},
   "source": [
    "Assuming the target function $f_T:\\mathbb{R}\\rightarrow \\mathbb{R}$, how to compare the following two functions $h_1(x)=2x$ and $h_2(x)=2x^2$ using the same performance metric $L(\\hat y, y)=(\\hat y -y)^2$? \n",
    "\n",
    "A sample of feature-target pairs are collected from certain population-1 and stored in the file `population_1.dat`. \n",
    "\n",
    "1. How many feature-target pairs are there in this sample?\n",
    "2. Construct a numpy array of predicted targets by $h_1$ and a numpy array of predicted targets by $h_2$. Name these two arrays `y1` and `y2` respectively.\n",
    "3. Construct a scattering plot of the loss of $h_1$ as a function of the collected features. Do the same for $h_2$ on the same figure.\n",
    "4. According to the scattering plot in Q3, does $h_1$ always outperform or underperform $h_2$ ?\n",
    "5. What is the empirical distribution of the feature in this population?\n",
    "6. Take into account of the feature distribution, can you guess which function, $h_1$ or $h_2$, performs better over the entire population?\n",
    "7. Come up with a measure that quantifies the performance of a function $h$ over the entire population with respect to a loss function. Apply this measure to $h_1$ and $h_2$ with the loss $L(\\hat y, y)=(\\hat y - y)^2$. Print the result, which one is better?\n",
    "\n",
    "Use the following code cell for this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "077fa2ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.clf()\n",
    "%matplotlib widget\n",
    "\n",
    "xs = np.linspace(0, 10, 101)\n",
    "\n",
    "h1 = lambda x: 2*x\n",
    "h2 = lambda x: 2*x**2\n",
    "\n",
    "loss_func = lambda y_true, y_pred: (y_true - y_pred)**2\n",
    "\n",
    "d1 = np.loadtxt('./data/projectile_1/population_1.dat')\n",
    "\n",
    "# how many feature-target pairs are there in this sample?\n",
    "\n",
    "# construct numpy arrays of predicted targets by h1 and h2\n",
    "# y1 = \n",
    "# y2 = \n",
    "\n",
    "# scattering plot of the loss of h1 as a function of the collected features in population 1\n",
    "\n",
    "# scattering plot of the loss of h2 as a function of the collected features in population 1\n",
    "\n",
    "# histogram of the feature in this population\n",
    "\n",
    "# print the result of the performance measure for h1 and h2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2a487e1",
   "metadata": {},
   "source": [
    "#### **Exercise 2.3.2: Comparing again for different population**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a793ec78",
   "metadata": {},
   "source": [
    "Now we investigate a different population `population_2.dat`. It is given that both population 1 and population 2 admit the same target function. Compare again the performance of $h_1$ and $h_2$ but this time over population 2. \n",
    "\n",
    "1. What is distribution of feature in population 2?\n",
    "2. Compare the two populations in terms of the feature distribution.\n",
    "3. Guess over the entire population 2, which one, $h_1$ or $h_2$, will perform better?\n",
    "4. Verify your guess using the measure defined in the previous exercise Q7.\n",
    "5. What can you draw as conclusion on how to properly comparing the performance different functions? \n",
    "\n",
    "Use the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02baf333",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "plt.clf()\n",
    "%matplotlib widget\n",
    "\n",
    "xs = np.linspace(0, 10, 101)\n",
    "\n",
    "h1 = lambda x: 2*x\n",
    "h2 = lambda x: 2*x**2\n",
    "\n",
    "loss_func = lambda y_true, y_pred: (y_true - y_pred)**2\n",
    "\n",
    "d2 = np.loadtxt('./data/projectile_1/population_2.dat')\n",
    "\n",
    "# what is distribution of feature in population 2?\n",
    "\n",
    "# compare the two populations in terms of the feature distribution, mean, standard deviation, etc.\n",
    "\n",
    "# verify your guess using the measure defined in the previous exercice Q7.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d0a5389",
   "metadata": {},
   "source": [
    "- **Expected loss** and **Empirical loss** -- The loss function only assign a degree of badness to an instance of feature-target pair when a hypothesis (function) $h$ is applied. However, we want to optimise our function (within the hypothesis space) such that, it performs well over all possibly encountered features. This is why we introduce the **Expected loss** to evaluate the \"badness\" of a funtion $h$ over the entire population of features. Since one disposes only the observed data (i.e. training data) to gain some knowledge about the population, one has to use the **empirical loss** defined as $$ \\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^n L(h(x_i), y_i)$$ where $(x_1,y_1),(x_2, y_2),\\ldots,(x_n,y_n)$ are observed feature-target pairs, to estimate the **expected loss**. \n",
    "\n",
    "  **Remark**\n",
    "   - It is however important to be clear that **empirical loss**, which estimates the performance in sample, is not **expected loss**, which measures the performance over the entire population. The only way to make these two quantities equal, is to make $n$ go to infinity (given that feature-target pairs are independently generated). \n",
    "   - This remark has important implications in how well a function $h$ optimised over the training samples can perform out of the sample, i.e. the expected loss. When optimisation is overly done to optimise the **empirical loss**, it can go against generalisation such that the **expected loss** is not optimal. A technique called **regularisation** is introduced for this issue, which will be discussed later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a57a15",
   "metadata": {},
   "source": [
    "- **Empirical loss** as a _landscape_. For a given set of observed data, (that is fixing the feature-target pairs), the **empirical loss** (as well as the **expected loss**) defines a multivariate function mapping hypothesis parameters to a scalar, that can be viewed as hyper-surface or a landscape.\n",
    "\n",
    "  > Taking the previous example of $\\mathcal{H}=\\{h(x)=ax^{p}| a\\in \\mathbb{R}, p\\in\\mathbb{Z}\\}$, for some given observed data $(x_1,y_1),(x_2, y_2),\\ldots,(x_n,y_n)$, the empirical loss reads explicitly $$ \\mathcal{L}(a, p) = \\frac{1}{n}\\sum_{i=1}^n L(ax_i^p, y_i) \\; .$$ Taking the value of $\\mathcal{L}(a,p)$ as the height associated to a localtion coordinate $(a,p)$, one obtains a landscape of \"mountains\" and \"valleys\" defined on the domain of $(a,p)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc6b6680",
   "metadata": {},
   "source": [
    "- **Learning algorithm** is the specific computational scheme to search for the optimal _parameters_ from the chosen hypothesis space. For the hypothesis space in the previous example with form $h(x)=ax^p$, a learning algorithm is a series of concrete computational operations that, when it finishes, returns the \"location\" $(a,p)$ at the bottom of some \"valley\" in the landscape of **empirical loss** (that ideally also sits at the bottom of **expected loss** landscape). \n",
    "\n",
    "  The process of searching the optimal function within the hypothesis space using some learning algorithm is called **training**, and the observed feature-target pairs involved in this process is called **training set**.\n",
    "\n",
    "  When an analytical solution is not possible, an iterative loop to approach the optimal parameters must be invoked in **Learning algorithm**. In this case, a learning algorithm can be viewed as a _dynamical system_ in the parameter space. \n",
    "\n",
    "  > A _dynamical system_, is a set of rules to update a state based only on the current state. Take $h(x)=ax^p$ as an example, a hypothesis (function) is completely determined by the pair $(a, p)$. A dynamical system on $(a,p)$ varies $a$ and $p$ solely based on $(a,p)$.  For example in each update step, we have the evolution $a\\rightarrow a+ \\Delta a,\\; p\\rightarrow p + \\Delta p$ where $\\Delta a = (a^2 + 3p)\\times \\ell$ and $\\Delta p = (-a+p)\\times \\ell$ with $\\ell$ setting the magnitude of each increment. Note that the increments $(\\Delta a, \\Delta p)$ are solely determined by the current state $(a,p)$. As such, some initial position $(a,p)$ draws a trajectory after multiple steps of update. In particular, when $\\ell\\rightarrow 0$, one end up with a system of differential equations.\n",
    "\n",
    "  A learning algorithm is such a dynamical system with a set of update rules that moves the state towards lower position in the landscape of empirical loss (e.g. $\\mathcal{L}(a,p)$) and eventually stops at the bottom of some valley. **Gradient descent** is the fundamental idea underlying most of the learning algorithms dealing with **empirical loss** in supervised machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d32e412a",
   "metadata": {},
   "source": [
    "#### **Exercise 2.3.4 Gradient Descent in 1 dimension**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6bff1d1",
   "metadata": {},
   "source": [
    "Assuming that we search the optimal function from the hypothesis space $\\mathcal{H} = \\{h(x)=kx|k\\in\\mathbb{R}\\}$. That is the optimal slope $k$ for minimizing some empirical loss. Assume also the empirical loss is given by $\\mathcal{L}(k) = k^2-5k+6$, that is a landscape defined on 1 dimensional space. We search for the optimal model identified with some $k^*$. \n",
    "\n",
    "1. How to find analytically the optimal $k^*$ for this empirical loss? What is the result?\n",
    "2. What is the derivative of $L$ with respect to $k$?\n",
    "3. What is the sign of the derivative when $k$ is smaller than the optimal $k^*$? and when $k>k^*$?\n",
    "4. How does the magnitude of the derivative vary when $k$ approaches $k^*$ from the left? and from the right?\n",
    "5. Set up a rule for updating $k$ with a small magnitude of increment $\\ell$, such that where ever is $k$, the increment will be in the direction to approach $k^*$ from the current $k$.\n",
    "6. How to make the increment rule adaptive such that, the increment will \"slow down\" in each one move when $k$ is getting closer to $k^*$? Hint: derivative magnitude.\n",
    "7. Implement this learning algorithm with Python with the help of the indications in the code cell below. \n",
    "8. Plot the $k$ as a function of the iteration steps until $k$ becomes more or less stable. Make trials with different initialiation of $k$ and different values of $\\ell$. \n",
    "9. Zoom into final part of $k$ verus iteration step, what do you see for very large number of total iterations?\n",
    "9. What is the effect of $\\ell$? Hint: in terms of steps to converge, and in terms of precision to $k^*$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72346d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Define the empirical loss function\n",
    "def empirical_loss(k):\n",
    "    return k**2 - 5*k + 6\n",
    "\n",
    "# Define the derivative of the empirical loss\n",
    "def derivative_loss(k):\n",
    "    pass\n",
    "\n",
    "# Define the update rule\n",
    "def update_rule(k, l):\n",
    "    pass\n",
    "\n",
    "# Implement the learning algorithm\n",
    "def learning_algorithm(k, l, num_steps):\n",
    "    record_k = [k]\n",
    "    for i in range(num_steps):\n",
    "        k = update_rule(k, l)\n",
    "        record_k.append(k)\n",
    "    return np.array(record_k)\n",
    "\n",
    "# Set initial parameters\n",
    "k = 0\n",
    "\n",
    "# Set the update magnitude\n",
    "l = 0.1\n",
    "\n",
    "# Set the number of steps\n",
    "num_steps = 100\n",
    "\n",
    "# Run the learning algorithm\n",
    "\n",
    "# Plot k as a function of the iteration steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f76aef6",
   "metadata": {},
   "source": [
    "#### **Exercise 2.3.3 Gradient Descent in real life**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cc70233",
   "metadata": {},
   "source": [
    "You are randomly dropped from a helicopter to somewhere in the Alps. Your goal is to arrive at the lowest point nearby.\n",
    "\n",
    "1. What did you actually do for reaching this goal? Explicitly describe your decision making process. (Ignore details such as small obstacles, plants and consider the landscape to be smooth)\n",
    "2. Consider the landscape to be smooth, and assume you can only see your surroundings, how to optimise your each step locally to follow the shortest path towards the lowest point nearby?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0243e17",
   "metadata": {},
   "source": [
    "### 2.4 Mini linear regression with \"scikit-learn\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ea6906",
   "metadata": {},
   "source": [
    "Now that we have clarified the machine learning task for the projectile problem and explored the training data, it is time to choose a **hypothesis space**, define a **loss function** and implement a **learning algorithm** to obtain the **trained model**, i.e. an optimal function that can make smart predictions about the distance of a projectile with the knowledge of the launching state in new experiments.\n",
    "\n",
    "For testing the performance of the trained model $h^*(\\cdot)$, we constructed the **test dataset** in `test_set_1.dat` and use the empirical loss of $h^*(\\cdot)$ applied on this test dataset as a survey for the true performance, i.e. the expected loss. For this survey to be faithful, it is crucial to guarantee **test set** and **training set** do not share any data points (feature-target pairs), otherwise the real performance could be overestimated. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5b6565d",
   "metadata": {},
   "source": [
    "To have a taste of how machine learning works, we are going to use the library \"scikit-learn\" (`import sklearn`) to realise machine learning of simple \"ordinary least square regression\", which means \n",
    "\n",
    "- For the **hypothesis space**, we choose a simple linear model that is $$\\mathcal{H}=\\{ h(v_x, v_y) = w_x v_x + w_y v_y + w_0| (w_x, w_y, w_0) \\in \\mathbb{R}^3\\}\\;.$$ Hence the training wind up to optimising the parameter triplet $(w_x, w_y, w_0)$ with respect to the empirical loss. The coefficients $(w_0, w_x, w_y)$ are called \"weights\" in a linear model.\n",
    "- For the **empirical loss**, we choose the **loss function** to be $$L(\\hat d, d ) = (\\hat d - d)^2$$ where $\\hat d$ and $d$ are the predicted and real distances respectively.\n",
    "- The optimal hypothesis for the above setting can be analysically solved by matrix inversion, that is the **learning algorithm** is realised via numerical matrix inversion."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88fe5b76",
   "metadata": {},
   "source": [
    "#### **Exercice 2.4.1**\n",
    "\n",
    "1. Take into account of the results in data exploration about the dependence of $d$ on $(v_x, v_y)$, what can you guess about the signs of $w_x$ and $w_y$ of an optimal linear function?\n",
    "\n",
    "2. Write the expression for the empirical loss, if $n$ feature-target pairs are given."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c388304f",
   "metadata": {},
   "source": [
    "#### Simple Ordinary Least Square (OLS) Regression with `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c20a8",
   "metadata": {},
   "source": [
    "We use the data in `training_set_1.dat` for training and `test_set_1.dat` for evaluating the trained model. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91febf65",
   "metadata": {},
   "source": [
    "The following code cell realises the entire process of OLS regression. Read, run and play with the code, for answering questions in the following exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e916bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "train1 = np.loadtxt('./data/projectile_1/training_set_1.dat')\n",
    "test1 = np.loadtxt('./data/projectile_1/test_set_1.dat')\n",
    "\n",
    "# Use training data (vx, vy) to predict d\n",
    "X_train = train1[:, :2]\n",
    "y_train = train1[:, 2]\n",
    "X_test = test1[:, :2]\n",
    "y_test = test1[:, 2]\n",
    "\n",
    "# Initialize OLS regression model\n",
    "ols = LinearRegression()\n",
    "\n",
    "# print weights before training\n",
    "print(\"Before training ------------\")\n",
    "try:\n",
    "    print(\"Coefficients before training:\", ols.coef_)\n",
    "    print(\"Intercept before training:\", ols.intercept_)\n",
    "except:\n",
    "    print(\"Weights before training: not available\")\n",
    "\n",
    "# Train OLS regression model\n",
    "ols.fit(X_train, y_train)\n",
    "\n",
    "# print weights after training\n",
    "print(\"After training ------------\")\n",
    "try:\n",
    "    print(\"Coefficients after training:\", ols.coef_)\n",
    "    print(\"Intercept after training:\", ols.intercept_)\n",
    "except:\n",
    "    print(\"Weights after training: not available\")\n",
    "\n",
    "# Predict on test set\n",
    "y_pred = ols.predict(X_test)\n",
    "\n",
    "# Compute performance measure (mean squared error)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error (on test set):\", mse)\n",
    "\n",
    "%matplotlib widget\n",
    "plt.clf()\n",
    "plt.scatter(y_test, y_pred, s=20, c='b', marker='o', alpha=0.5)\n",
    "plt.plot([100, 3000], [100, 3000], 'k--')\n",
    "plt.show()\n",
    "\n",
    "# Implement the empirical loss\n",
    "def empirical_loss(X, y, model):\n",
    "    pass\n",
    "\n",
    "    # to complete"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc36a77",
   "metadata": {},
   "source": [
    "#### **Exercise 2.4.2** OLS regression with `sklearn`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f74c5f6",
   "metadata": {},
   "source": [
    "1. What is the reference name (or object) that carries the information about the linear model space, the empirical loss and the training method? How is the this object created by code?\n",
    "2. After the training and testing datasets are loaded and prepared, what are the two crucial calls / steps to obtain a trained simple linear model?\n",
    "3. Which line of code carries the actual training process? \n",
    "4. How are training data fed to the training process? Importantly, what do the rows and columns represents in `X_train` and `y_train`? What if we have $M$ feature-target pairs where each feature compresses $st input variables, and each target compresses $t$ variables?\n",
    "5. After training, which values do the weights take for $w_x, w_y, w_0$ respectively?\n",
    "6. What the plot should look like if the trained model makes perfect predictions?\n",
    "7. Complete the function `empirical_loss` taking `X`, `y` feature-target pairs in the same format as `y_test`, `y_pred` and a model object `model` as its arguments to return the empirical loss, without using `mean_squared_loss`. Verify it returns the same result as `mse`.\n",
    "8. Use the `empirial_loss` to evaluate the trained model on the training set. How does it compare to the empirical loss on the test set? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62c693dc",
   "metadata": {},
   "source": [
    "#### **Exercise 2.4.3** Create your own linear model with a `class`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f39f5d",
   "metadata": {},
   "source": [
    "Class is a core concept in Python, since in Python everything is an object of certain \"type\". This \"type\" refers to some \"class\". For example, run the following code cell, and you will see that all buit-in data types are certain classes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da68295f",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = int(0)\n",
    "print(type(a))\n",
    "\n",
    "b = float(0.0)\n",
    "print(type(b))\n",
    "\n",
    "c = str('0')\n",
    "print(type(c))\n",
    "\n",
    "d = list([0])\n",
    "print(type(d))\n",
    "\n",
    "e = list()\n",
    "print(type(e))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43737083",
   "metadata": {},
   "source": [
    "In the above code cell each variable name (also called \"reference\") represent an object or an instance of certain class. For example, `a` is an instance or object of `int` class, `d` and `e` are two different objects or instances of the same class `list`. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17fdf9eb",
   "metadata": {},
   "source": [
    "A class is a blueprint or meta-form for creating concrete objects. Here is a piece of code to declare a `my_linear_model` class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da3a3c01",
   "metadata": {},
   "outputs": [],
   "source": [
    "class my_linear_model:\n",
    "    def __init__(self, w_x=0.0, w_y=0.0, w_0=0.0):\n",
    "        self.w_x = w_x\n",
    "        self.w_y = w_y\n",
    "        self.w_0 = w_0\n",
    "    \n",
    "    def predict(self, X):\n",
    "        assert X.ndim == 2 and X.shape[1] == 2, \"X must be a 2D array with 2 columns\"\n",
    "        return self.w_x*X[:, 0] + self.w_y*X[:, 1] + self.w_0\n",
    "\n",
    "my_model1 = my_linear_model()\n",
    "my_model2 = my_linear_model(1.0, 1.0, 1.0)\n",
    "\n",
    "X = np.array([[1.0, 2.0], [2.0, 3.0], [3.0, 4.0]])\n",
    "\n",
    "y_pred1 = my_model1.predict(X)\n",
    "y_pred2 = my_model2.predict(X)\n",
    "\n",
    "print('my_model1.predict(X)', y_pred1)\n",
    "print('my_model2.predict(X)', y_pred2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba586d6c",
   "metadata": {},
   "source": [
    "1. Craete an instance `my_traine_model` of class `my_linear_model` with the weights initialised using the trained model weights.\n",
    "2. Make a prediction using this instance with `X_test`, and compare with the prediction of trained model `ols`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a0dffdb",
   "metadata": {},
   "source": [
    "### 2.5 Linear Models and Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33c7b558",
   "metadata": {},
   "source": [
    "#### 2.5.1 Linear model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93581692",
   "metadata": {},
   "source": [
    "A **linear model** is one of the simplest and most fundamental types of machine learning models.\n",
    "It assumes that the **target variable** ( $y$ ) can be expressed as a **linear combination** of the **input variables** ( $x_1, x_2, \\ldots, x_n$ ):\n",
    "\n",
    "$$\n",
    "y = w_0 + w_1 x_1 + w_2 x_2 + \\cdots + w_n x_n \n",
    "$$\n",
    "\n",
    "where:\n",
    "\n",
    "* $w_0$ is the **bias (intercept)** term,\n",
    "* $w_1, \\ldots, w_n$  are the **model coefficients (weights)**,\n",
    "\n",
    "In vector form:\n",
    "\n",
    "$$\n",
    "y = \\mathbf{w}^\\top \\mathbf{x} + w_0\n",
    "$$\n",
    "\n",
    "This formulation applies to both **regression** (predicting scalar outcomes) and **classification** (predicting categories, often via logistic or softmax functions).\n",
    "\n",
    "\n",
    "> **Why Linear Models Matter**\n",
    ">\n",
    "> Linear models are conceptually simple but extremely powerful:\n",
    "> \n",
    "> * They are **easy to interpret** — each weight directly shows how much a feature influences the output.\n",
    "> * They are **computationally efficient** — training involves solving convex optimization problems (often via least squares or gradient descent).\n",
    "> * They serve as a **baseline model** — often the first model tested before more complex nonlinear ones.\n",
    "> * Many nonlinear models (like neural networks) can be viewed as compositions of **linear transformations plus nonlinearities**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e86eb163",
   "metadata": {},
   "source": [
    "#### 2.5.2 Feature Engineering and the Power of Linear Models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae0b6aab",
   "metadata": {},
   "source": [
    "Because linear models only learn *linear relationships*, the expressiveness of the model depends heavily on the **features** provided. That’s where **feature engineering** becomes critical.\n",
    "\n",
    "**Feature engineering** means transforming raw feature input variables into informative features that better capture the underlying relationships between inputs and outputs. For linear models, this can include:\n",
    "\n",
    "* **Polynomial features:** ( $x, x^2, x^3, \\ldots$ ) allow modeling nonlinear trends.\n",
    "* **Interaction terms:** ( $x_1 \\times x_2$ ) capture nonlinearity between features.\n",
    "* **Normalization/scaling:** ensures all features contribute proportionally (important for gradient-based optimization).\n",
    "* **Feature selection:** removing redundant or irrelevant variables improves generalization.\n",
    "\n",
    "With the right feature transformations, a linear model can approximate surprisingly complex patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eff713b",
   "metadata": {},
   "source": [
    "Formally for a feature of $n$ input variables $\\mathbf{x} = (x_1, x_2, \\ldots, x_n)$ (or also commonly called \"$n$ features for one input\"), **feature engineering** consists in constructing $m$ complex features $\\phi_1(\\mathbf{x}), \\phi_2(\\mathbf{x}), \\ldots, \\phi_m(\\mathbf{x})$ from the $n$ raw features, with usually $m>n$, for capturing non-linearity in the target function. Each $\\phi_i(\\mathbf{x})$ is a function of all raw input variables $x_1, x_2, \\ldots, x_n$. \n",
    "\n",
    "A more expressive linear model now pocesses $m$ weights and reads \n",
    "\n",
    "$$\n",
    "y = w_1 \\phi_1(\\mathbf x) + w_2 \\phi_2(\\mathbf x) + \\cdots + w_m \\phi_m(\\mathbf x) + w_0\n",
    "$$\n",
    "\n",
    "or in a vector form\n",
    "\n",
    "$$\n",
    "y = \\mathbf{w}^T\\cdot \\mathbf{\\phi}(\\mathbf{x}) + w_0\\;.\n",
    "$$\n",
    "\n",
    "As such the linear regression is transformed into a new one but always **linear** in the tuning parameters $\\mathbf{w}$.\n",
    "\n",
    "For example, **polynomial features** $\\phi_p(x)$ for a single input variable $x$ reads $\\phi_p(x) = x^p$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ce17c57",
   "metadata": {},
   "source": [
    "#### **Exercice 2.5.1 More data exploration for the projectile**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a12bb268",
   "metadata": {},
   "source": [
    "We are going to dive deeper in data exploration with the `training_set_1.dat`. Using a code cell below to investigate the following questions.\n",
    "\n",
    "1. Is the dependence of the distance $d$ linear on each of $v_x$ and $v_y$, when the other is fixed?\n",
    "2. How does the dependence of $d$ on $v_x$ for a fixed $v_y$ varies when varying $v_y$? \n",
    "3. The same question as Q2 except exchanging $v_x$ and $v_y$.\n",
    "4. What kind of form can you guess for the target function?\n",
    "5. Is your guess consistent with the results (especially the colored scattering plot) in the previous data exploration?\n",
    "6. Is your guess consistent with the point cloud shape in the OLS regression?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2084590f",
   "metadata": {},
   "source": [
    "#### **Exercice 2.5.2 OLS regression with Feature engineering for projectile**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6dc6f96",
   "metadata": {},
   "source": [
    "Based on the previous OLS results and the analysis above, a simple linear form $h(v_x, v_y) = w_x v_x + w_y v_y + w_0$ does not capture the complexity of the target function. We are going to transform the raw featuers $v_x,\\; v_y$ to higher order features. Each new feature $\\phi_{p,q}$ is of form $\\phi_{p,q} = v_x^pv_y^q$, with $p+q$ from $1$ up to $3$.\n",
    "\n",
    "1. For $p+q=1, 2, 3$, list all possible features $phi$. In such way, we transform 2 input variables into a feature of $m$ variables, what is $m$?\n",
    "2. Using a code cell to load prepare `X_train`, `y_train`, `X_test`, `y_test` as the OLS regression, and then construct, using the same format of `X_train` or `X_test`, `Xnew_train` and `Xnew_test` of the new features. Keep a track of which column in the input matrix corresponding to which $(p,q)$ pair.\n",
    "3. Redo the linear regression, this time with an linear model object `ols_new`, with the new features.\n",
    "4. Make a scattering plot of $y_pred_new$ versus $y_test$, in the same figure to compare with OLS.\n",
    "5. Evaluate the empirical loss of the newly predicted target on the test dataset. How does it compare with previous OLS? Is it consistent with the could points shape changes?\n",
    "6. Investigate the coefficients for each new feature index by $(p,q)$. Compare with the coefficients in the case without feature engineering, what is the origin of the improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e918c670",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "train1 = np.loadtxt('./data/projectile_1/training_set_1.dat')\n",
    "test1 = np.loadtxt('./data/projectile_1/test_set_1.dat')\n",
    "\n",
    "# Use training data (vx, vy) to predict dx\n",
    "X_train = train1[:, :2]\n",
    "y_train = train1[:, 2]\n",
    "X_test = test1[:, :2]\n",
    "y_test = test1[:, 2]\n",
    "\n",
    "# Construct the new features\n",
    "# X_train_new\n",
    "# X_test_new\n",
    "\n",
    "# Initialize OLS regression model\n",
    "# ols = \n",
    "# ols_new = \n",
    "\n",
    "# Train OLS regression model with both the raw features and the new features\n",
    "\n",
    "# Predict on test set\n",
    "# y_pred = \n",
    "# y_pred_new = \n",
    "\n",
    "# Compute performance measure (mean squared error)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "mse_new = mean_squared_error(y_test, y_pred_new)\n",
    "\n",
    "print(\"Mean Squared Error     (on test set):\", mse)\n",
    "print(\"Mean Squared Error new (on test set):\", mse_new)\n",
    "\n",
    "# Investigate the model coefficients for the new features\n",
    "\n",
    "# Scattering plot of y_pred_new versus y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "276e08de",
   "metadata": {},
   "source": [
    "### 2.6 Generalisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "366af13d",
   "metadata": {},
   "source": [
    "So far, we have discussed key elements involved in obtaining a reasonably optimized machine learning model for a given task. Here, we will explore factors in the training process that may affect **generalization**.\n",
    "\n",
    "From a practical point of view, **generalization** refers to a model’s performance on unseen data — that is, the empirical loss on new (unseen) samples. Conceptually, **generalization** represents the *expected loss* over the entire *population* of feature–target pairs. The term *population* here refers to the underlying probability distribution of the variables or objects under consideration.\n",
    "\n",
    "* The difficulty in achieving good **generalization** arises from the fact that training is always performed on a finite dataset, which inevitably differs from the true **population**. As a result, training produces a model optimized for the empirical loss on the training data, which is, in principle, different from the model that would minimize the **expected loss** over the true population. Hence, to fully exploit the potential of a given hypothesis space, the best approach is to use as much data as possible.\n",
    "\n",
    "  But how much data is enough? The answer is that we don’t actually know, because the underlying probability distribution is unknown. This is precisely why data collection is necessary for training a model in the first place. Without knowing the true distribution, it is difficult to determine how much data is sufficient to achieve a desired level of uncertainty tolerance.\n",
    "\n",
    "* There is another layer of difficulty in achieving good **generalization**. A hypothesis space has a certain degree of *flexibility* or *expressiveness*. For example, a linear model ( $h(x) = w_1x + w_0$ ) is less flexible than a quadratic model ( $h(x) = w_2x^2 + w_1x + w_0$ ); the latter has greater expressive power. Flexibility, or expressiveness, of a model is also referred to as its *complexity*. In principle, we want a model to be sufficiently flexible or complex so that it can learn a wide range of possible target functions.\n",
    "\n",
    "  However, since a flexible model is optimized on a finite training dataset — which inevitably contains specific peculiarities — it may become overly tailored to that dataset. In other words, the model may learn random features that do not exist in the true population but happen to appear in the training set. This phenomenon, called **overfitting**, results from the combination of limited data and model flexibility.\n",
    "\n",
    "For these reasons, when data is limited, it is common practice to split the available dataset into a **training set** and a **testing set** in order to:\n",
    "\n",
    "1. use as much data as possible for training, to better approximate the true population, and\n",
    "2. reserve a separate set, excluded from training, to serve as an indicator of real-world performance and a safeguard against overfitting."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e372bde0",
   "metadata": {},
   "source": [
    "#### **Exercise** 2.6.1 The effect of the size of training set"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "685aea8c",
   "metadata": {},
   "source": [
    "In this exercise, we are going to assume the empirical loss on the testing dataset `test_set_1.dat` is a relatively a good measure of the true performance. So far, we are using the entire training dataset to train a ordinary linear model. The aim of this exercice is to investigate how the size of training dataset affect a trained model's true performace. We are going to use the feature engineered linear model in the previous section as our hypothesis space.\n",
    "\n",
    "Use the code cell below for the following exercises\n",
    "1. Construct the engineered features as the last exercice for both training and testig set.\n",
    "2. Create an array of training dataset sizes that are equally log spaced using `numpy.logspace` from 10 to 10000. This array should include 9 different sizes with 10 and 1000 included.\n",
    "3. Make a for loop iterating over the size array. For each training data size, generate trainig set of the corresponding size by retire uniformly randomly from the original training data set (loaded from `training_set_1.dat`), using `numpy.random.choice`. \n",
    "4. For each training set (inside the for loop):\n",
    "  - train a OLS regression model\n",
    "  - make a prediction using the testing input, evaluate the testing prediction, and store it into a list\n",
    "  - make a prediction using the training input, evaluate the training prediction, and store it into a list\n",
    "  - store the weights and the intercept into a list\n",
    "5. Plot the training loss and \"true\" loss as a function of the training dataset sizes. Set the sizes to log scale. \n",
    "6. Plot the the weights and intercepts for different training dataset sizes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48fac0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "train1 = np.loadtxt('./data/projectile_1/training_set_1.dat')\n",
    "test1 = np.loadtxt('./data/projectile_1/test_set_1.dat')\n",
    "\n",
    "X_train = train1[:, :2]\n",
    "y_train = train1[:, 2]\n",
    "X_test = test1[:, :2]\n",
    "y_test = test1[:, 2]\n",
    "\n",
    "# Feature engineering\n",
    "X_train_new = np.zeros((len(X_train), 9))\n",
    "X_test_new = np.zeros((len(X_test), 9))\n",
    "\n",
    "# Training set size array\n",
    "n_train = np.logspace(1, 3, 9).astype(int)\n",
    "\n",
    "Parameters = []\n",
    "Training_loss = []\n",
    "Testing_loss = []\n",
    "# Training loop\n",
    "for i, n in enumerate(n_train):\n",
    "    # Generate training set\n",
    "    idx = np.random.choice( replace=False)\n",
    "    X_train_sub = X_train_new[idx]\n",
    "    y_train_sub = y_train[idx]\n",
    "    \n",
    "    # Train OLS regression model\n",
    "    \n",
    "    # Make a prediction using the testing input\n",
    "    \n",
    "    Testing_loss.append(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    # Make a prediction using the training input\n",
    "    \n",
    "    Training_loss.append(mean_squared_error(y_train_sub, y_pred_train_sub))\n",
    "    \n",
    "    # Store the weights and the intercept\n",
    "    Parameters.append(ols.coef_)\n",
    "\n",
    "# Plot the training loss and \"true\" loss as a function of the training dataset sizes\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the weights and the intercept as a function of the training dataset sizes\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "\n",
    "# plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55322de",
   "metadata": {},
   "source": [
    "#### **Battling overfitting with Regularisation**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9cfdcb7",
   "metadata": {},
   "source": [
    "Given the risk of overfitting due to the combination of finite training set and model flexibility, we introduce a **regularisation** term (also called \"panelty\") into the object of optimisation. The penalty is a function denoted $\\mathcal{L}_R()$ mapping the tuning parameters of the machine learning model to a scalar. \n",
    "\n",
    "The property of this penalty term is that it favors model parameters that makes the model function less complex or \"featureless\", or just simple. For example, a common penalty form for a linear model $y=\\sum_{i=1}^n w_i x_i + w_0$ is quadratic form $\\sum_{i=1}^n w_i^2$, which favors the weights to be small when a learning algorithm searchs for minimizing it. \n",
    "\n",
    "Before the object of optimisation has always been only the empirical loss on the training set. With a panelty term, the object of optimisation becomes $$ \\text{Empirical loss} + \\alpha \\text{Penalty} $$ where $\\alpha$ is introduced as a _hyper-parameter_ called \"penalty strength\", to set the influential power of the penalty over the optimisation process. For a linear regression using the squared loss, the object of optimisation becomes \n",
    "\n",
    "$$\n",
    "\\tilde{ \\mathcal{L} }= \\frac{1}{N} \\sum_{i=1}^N (\\mathbf{w}^T\\cdot \\mathbf{x}_i + w_0 - y_i)^2 + \\alpha \\mathbf{w}^T\\cdot \\mathbf{w}\n",
    "$$\n",
    "\n",
    "where $N$ the number of data points. When $\\alpha$ is very large, all weigths tend to be zero, and when $\\alpha$ is small it recovers the ordinary least square regression. Note also that $\\alpha$ such defined represents the power of penalty per data point. This form of linear regression is called \"Ridge regression\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8939d9ff",
   "metadata": {},
   "source": [
    "#### **Exercice 2.6.2 Ridge Linear Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c97bc8",
   "metadata": {},
   "source": [
    "The Ridge linear regression is also implemented by scikit-learn. Here is the way to call it with some $\\alpha$:\n",
    "```\n",
    "from sklearn.linear_models import Ridge\n",
    "ridge_model = Ridge(alpha=1.0)\n",
    "```\n",
    "The rest remains the same as OLS regression. Note however, in the scikit-learn implementation, the setting of alpha is not normalised by the training size, meaning, to have a similar penalty strength for different training sample sizes $N$, the `alpha` in the `Ridge()` should be set as $N\\times alpha$.\n",
    "\n",
    "From the previous exercice, the training sample size $100$ shows a moderate degree of overfitting. We are going to set up a training sample of 100 data points and use this training set to perform Ridge regression for different values of $\\alpha$ to see its effect.\n",
    "\n",
    "1. Set up a traingin sample of 100 samples randomly drawn from the original training set and do the feature engineering to obtain `X_train_new` and `X_test_new`.\n",
    "2. Construct a series of values of $\\alpha$ ranging from $10^{-5}$ to $10^{4}$ with log scale spacing, call it `Alphas`. Suggestion: 37 values with $10^{-5}$ and $10^4$ included.\n",
    "3. Loop over different values of $\\alpha$, and in each loop:\n",
    "  - Perform the Ridge lieanr regression with a normalized penalty strengh $\\alpha$, i.e. `Ridge(alpha=N*Alphas[i])`\n",
    "  - Make a prediction with the test data, evaluate the result and store it into a list.\n",
    "  - Make a prediction with the training data, evaluate the results and store it into a list.\n",
    "  - Store the fitted weights and intercept into a list\n",
    "4. Plot the weights as a function of $\\alpha$, using log scale for $\\alpha$.\n",
    "5. Plot the training loss and the test loss as function of $\\alpha$.\n",
    "6. Which value of $\\alpha$ is optimal? Why? \n",
    "7. What are the weights at the optimal $\\alpha$? What is the physical interpretation?\n",
    "\n",
    "Use the following code cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a2db5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.6.2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "train1 = np.loadtxt('./data/projectile_1/training_set_1.dat')\n",
    "test1 = np.loadtxt('./data/projectile_1/test_set_1.dat')\n",
    "\n",
    "X_train = train1[:, :2]\n",
    "y_train = train1[:, 2]\n",
    "X_test = test1[:, :2]\n",
    "y_test = test1[:, 2]\n",
    "\n",
    "# Feature engineering\n",
    "X_train_new = np.zeros((len(X_train), 9))\n",
    "X_test_new = np.zeros((len(X_test), 9))\n",
    "\n",
    "# Construct a training set of 100 samples\n",
    "N = 100\n",
    "idx = np.random.choice(len(X_train_new), N, replace=False)\n",
    "X_train_sub = X_train_new[idx]\n",
    "y_train_sub = y_train[idx]\n",
    "\n",
    "# Set up a series of alpha values\n",
    "Alphas = np.logspace(-7, 7, 41)\n",
    "\n",
    "Training_loss = []\n",
    "Testing_loss = []\n",
    "Weights = []\n",
    "Intercepts = []\n",
    "for i, alpha in enumerate(Alphas):\n",
    "    # Perform Ridge regression\n",
    "    \n",
    "    \n",
    "    # Make a prediction with the test data\n",
    "    \n",
    "    \n",
    "    # Evaluate the result and store it into a list\n",
    "    \n",
    "    \n",
    "    # Make a prediction with the training data\n",
    "    \n",
    "    \n",
    "    # Evaluate the result and store it into a list\n",
    "    \n",
    "    \n",
    "    # Store the fitted weights and intercept\n",
    "    Weights.append(ridge_model.coef_)\n",
    "    Intercepts.append(ridge_model.intercept_)\n",
    "\n",
    "# Plot the weights as a function of alpha\n",
    "Weights = np.array(Weights)\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the training loss and the test loss as function of alpha\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(Alphas, Training_loss, label='Training loss')\n",
    "plt.plot(Alphas, Testing_loss, label='Testing loss')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65ec3050",
   "metadata": {},
   "source": [
    "## 2. Project-2: horizontal distance from velocity norm and angle $(v,\\theta)$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a306d8ea",
   "metadata": {},
   "source": [
    "Apply the concepts and techniques reviewed in Project-1 to Project-2: predict the horizontal distance using the magnitude of the initial velocity ($v$) and the launch angle ($\\theta$). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d083b044",
   "metadata": {},
   "source": [
    "Combining markdown text and code cell to realize this project with the help of scikit-learn. Here is a suggested structure to proceed:\n",
    "\n",
    "- Define the target function.\n",
    "- Explore the data `training_set_2.dat` and `test_set_2.dat`.\n",
    "- Define a linear model with a proper feature engineering using indications from data exploration.\n",
    "- Do the Ridge linear regression\n",
    "- Find the optimal hyperparameter, and the associated weights\n",
    "- Give a physical interpretation of the machine learning results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8a8d8e",
   "metadata": {},
   "source": [
    "## Supplements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaf9750f",
   "metadata": {},
   "outputs": [],
   "source": [
    "## code for generating training and test sets ./data/projectile_1\n",
    "g = 9.81\n",
    "\n",
    "def distance_vx_vy(vx, vy):\n",
    "    return vx*(vy/g)*2\n",
    "    \n",
    "def distance_v_alpha(v, alpha):\n",
    "    return v*np.sin(2*alpha)*(v/g)\n",
    "\n",
    "n_train = 10000\n",
    "n_test = 2000\n",
    "\n",
    "vx = np.random.normal(80, 15, n_train + n_test)    \n",
    "vy = np.random.normal(80, 15, n_train + n_test)\n",
    "\n",
    "v = np.sqrt(vx**2 + vy**2)\n",
    "alpha = np.arctan2(vy, vx)\n",
    "\n",
    "dx_std = 10\n",
    "dx = vx*(vy/g)*2 + np.random.normal(0, dx_std, n_train + n_test)\n",
    "\n",
    "\n",
    "train1 = np.zeros((n_train, 3))\n",
    "train1[:,0] = vx[:n_train]\n",
    "train1[:,1] = vy[:n_train]\n",
    "train1[:,2] = dx[:n_train]\n",
    "\n",
    "test1 = np.zeros((n_test, 3))\n",
    "test1[:,0] = vx[n_train:n_train+n_test]\n",
    "test1[:,1] = vy[n_train:n_train+n_test]\n",
    "test1[:,2] = dx[n_train:n_train+n_test]\n",
    "\n",
    "train2 = np.zeros((n_train, 3))\n",
    "train2[:,0] = v[:n_train]\n",
    "train2[:,1] = alpha[:n_train]\n",
    "train2[:,2] = dx[:n_train]\n",
    "\n",
    "test2 = np.zeros((n_test, 3))\n",
    "test2[:,0] = v[n_train:n_train+n_test]\n",
    "test2[:,1] = alpha[n_train:n_train+n_test]\n",
    "test2[:,2] = dx[n_train:n_train+n_test]\n",
    "\n",
    "\n",
    "# np.savetxt('./data/projectile_1/training_set_1.dat', train1, header='vx vy dx')\n",
    "# np.savetxt('./data/projectile_1/training_set_2.dat', train2, header='v alpha dx')\n",
    "# np.savetxt('./data/projectile_1/test_set_1.dat', test1, header='vx vy dx')\n",
    "# np.savetxt('./data/projectile_1/test_set_2.dat', test2, header='v alpha dx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0df0bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "sc = plt.scatter(vx[:1000], vy[:1000], c=dx[:1000], marker='o')\n",
    "plt.colorbar(sc, label='dx')\n",
    "plt.xlabel('vx')\n",
    "plt.ylabel('vy')\n",
    "plt.title('vy vs vx colored by dx (first 1000 points)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82b39c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(vx, dx, 'o', alpha=0.2, mec='none' )\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('vx')\n",
    "plt.ylabel('dx')\n",
    "plt.title('dx vs vx')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80305028",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(0, 2, 101)\n",
    "y1 = 2*x\n",
    "y2 = 2*x**2\n",
    "z = 2*x**1.5\n",
    "\n",
    "plt.plot(x, (y1-z)**2, label='y1')\n",
    "plt.plot(x, (y2-z)**2, label='y2')\n",
    "plt.legend()\n",
    "\n",
    "s1 = np.random.exponential(0.1,1000)\n",
    "t1 = 2*s1**1.5 \n",
    "s2 = np.random.exponential(1.0,1000)\n",
    "t2 = 2*s2**1.5\n",
    "\n",
    "plt.plot(s1, t1, 'o', alpha=0.2, mec='none')\n",
    "plt.plot(s2, t2, 'o', alpha=0.2, mec='none')\n",
    "plt.show()\n",
    "\n",
    "s = s1\n",
    "t = t1\n",
    "h1 = 2*s\n",
    "h2 = 2*s**2\n",
    "plt.figure()\n",
    "plt.plot(t, h1, 'o', alpha=0.2, mec='none')\n",
    "plt.plot(t, h2, 'o', alpha=0.2, mec='none')\n",
    "plt.show()\n",
    "\n",
    "print('h1', np.mean(np.abs(h1-t)))\n",
    "print('h2', np.mean(np.abs(h2-t)))\n",
    "\n",
    "d1 = np.zeros((len(s1), 2))\n",
    "d2 = np.zeros((len(s2), 2))\n",
    "d1[:,0] = s1\n",
    "d1[:,1] = t1\n",
    "d2[:,0] = s2\n",
    "d2[:,1] = t2\n",
    "# np.savetxt('./data/projectile_1/population_1.dat', d1, header='x y')\n",
    "# np.savetxt('./data/projectile_1/population_2.dat', d2, header='x y')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d201a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "## exercise 2.6.1\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "train1 = np.loadtxt('./data/projectile_1/training_set_1.dat')\n",
    "test1 = np.loadtxt('./data/projectile_1/test_set_1.dat')\n",
    "\n",
    "X_train = train1[:, :2]\n",
    "y_train = train1[:, 2]\n",
    "X_test = test1[:, :2]\n",
    "y_test = test1[:, 2]\n",
    "\n",
    "# Feature engineering\n",
    "X_train_new = np.zeros((len(X_train), 9))\n",
    "X_test_new = np.zeros((len(X_test), 9))\n",
    "\n",
    "ci = 0\n",
    "for s in range(1, 4):\n",
    "    for p in range(s+1):\n",
    "        X_train_new[:, ci] = X_train[:, 0]**(s-p) * X_train[:, 1]**p\n",
    "        X_test_new[:, ci] = X_test[:, 0]**(s-p) * X_test[:, 1]**p\n",
    "        # print('s.{} p.{}, q.{}'.format(s, p, s-p))\n",
    "        ci += 1\n",
    "\n",
    "# Training set size array\n",
    "n_train = np.logspace(1, 3, 9).astype(int)\n",
    "\n",
    "Parameters = []\n",
    "Training_loss = []\n",
    "Testing_loss = []\n",
    "# Training loop\n",
    "for i, n in enumerate(n_train):\n",
    "    # Generate training set\n",
    "    idx = np.random.choice(len(X_train_new), n, replace=False)\n",
    "    X_train_sub = X_train_new[idx]\n",
    "    y_train_sub = y_train[idx]\n",
    "    \n",
    "    # Train OLS regression model\n",
    "    ols = LinearRegression()\n",
    "    ols.fit(X_train_sub, y_train_sub)\n",
    "    \n",
    "    # Make a prediction using the testing input\n",
    "    y_pred_test = ols.predict(X_test_new)\n",
    "    Testing_loss.append(mean_squared_error(y_test, y_pred_test))\n",
    "    \n",
    "    # Make a prediction using the training input\n",
    "    y_pred_train_sub = ols.predict(X_train_sub)\n",
    "    Training_loss.append(mean_squared_error(y_train_sub, y_pred_train_sub))\n",
    "    \n",
    "    # Store the weights and the intercept\n",
    "    Parameters.append(ols.coef_)\n",
    "\n",
    "# Plot the training loss and \"true\" loss as a function of the training dataset sizes\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(n_train, Training_loss, label='Training loss')\n",
    "plt.plot(n_train, Testing_loss, label='Testing loss')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Plot the weights and the intercept as a function of the training dataset sizes\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "pmt = np.array(Parameters)\n",
    "for i in range(pmt.shape[1]):\n",
    "    plt.plot(n_train, pmt[:, i], label='w_{}'.format(i+1))\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc84503e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercise 2.6.2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "%matplotlib widget\n",
    "\n",
    "train1 = np.loadtxt('./data/projectile_1/training_set_1.dat')\n",
    "test1 = np.loadtxt('./data/projectile_1/test_set_1.dat')\n",
    "\n",
    "X_train = train1[:, :2]\n",
    "y_train = train1[:, 2]\n",
    "X_test = test1[:, :2]\n",
    "y_test = test1[:, 2]\n",
    "\n",
    "# Feature engineering\n",
    "X_train_new = np.zeros((len(X_train), 9))\n",
    "X_test_new = np.zeros((len(X_test), 9))\n",
    "\n",
    "ci = 0\n",
    "for s in range(1, 4):\n",
    "    for p in range(s+1):\n",
    "        X_train_new[:, ci] = X_train[:, 0]**(s-p) * X_train[:, 1]**p\n",
    "        X_test_new[:, ci] = X_test[:, 0]**(s-p) * X_test[:, 1]**p\n",
    "        # print('s.{} p.{}, q.{}'.format(s, p, s-p))\n",
    "        ci += 1\n",
    "\n",
    "N = 100\n",
    "idx = np.random.choice(len(X_train_new), N, replace=False)\n",
    "X_train_sub = X_train_new[idx]\n",
    "y_train_sub = y_train[idx]\n",
    "\n",
    "\n",
    "Alphas = np.logspace(-7, 7, 41)\n",
    "\n",
    "Training_loss = []\n",
    "Testing_loss = []\n",
    "Weights = []\n",
    "Intercepts = []\n",
    "for i, alpha in enumerate(Alphas):\n",
    "    ridge_model = Ridge(alpha=N*alpha)\n",
    "    ridge_model.fit(X_train_sub, y_train_sub)\n",
    "    y_pred_test = ridge_model.predict(X_test_new)\n",
    "    y_pred_train = ridge_model.predict(X_train_sub)\n",
    "    Training_loss.append(mean_squared_error(y_train_sub, y_pred_train))\n",
    "    Testing_loss.append(mean_squared_error(y_test, y_pred_test))\n",
    "    Weights.append(ridge_model.coef_)\n",
    "    Intercepts.append(ridge_model.intercept_)\n",
    "\n",
    "Weights = np.array(Weights)\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "for i in range(Weights.shape[1]):\n",
    "    plt.plot(Alphas, Weights[:,i], label='w{}'.format(i))\n",
    "# plt.plot(Alphas, Intercepts, label='Intercepts')\n",
    "plt.xscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "plt.figure()\n",
    "plt.clf()\n",
    "plt.plot(Alphas, Training_loss, label='Training loss')\n",
    "plt.plot(Alphas, Testing_loss, label='Testing loss')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
